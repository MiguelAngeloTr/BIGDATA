{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelAngeloTr/BIGDATA/blob/main/C2/ProyectoFinal/Proyectofinal1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estudiantes:\n",
        "\n",
        "Miguel Angel Jimenez Trochez - 2215407\n",
        "Juan Pablo Castaño  - 2215929\n",
        "Christian David Cardenaz  - 2215689\n"
      ],
      "metadata": {
        "id": "QnjQHAivXfWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pyspark\n",
        "!pip install -q findspark\n",
        "!pip install kagglehub[pandas-datasets]"
      ],
      "metadata": {
        "id": "VRzfBciSJGyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf19856d-5c71-4675-f2e1-7ed2e1216bdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kagglehub[pandas-datasets]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub[pandas-datasets]) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask==2024.12.1 distributed==2024.12.1 --force-reinstall\n"
      ],
      "metadata": {
        "id": "304dsAiBFqrz",
        "outputId": "dff8cc6d-f8e6-4e7a-a1c5-618c3a11e4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dask==2024.12.1\n",
            "  Downloading dask-2024.12.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting distributed==2024.12.1\n",
            "  Downloading distributed-2024.12.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting click>=8.1 (from dask==2024.12.1)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting cloudpickle>=3.0.0 (from dask==2024.12.1)\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting fsspec>=2021.09.0 (from dask==2024.12.1)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting packaging>=20.0 (from dask==2024.12.1)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting partd>=1.4.0 (from dask==2024.12.1)\n",
            "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyyaml>=5.3.1 (from dask==2024.12.1)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting toolz>=0.10.0 (from dask==2024.12.1)\n",
            "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting importlib_metadata>=4.13.0 (from dask==2024.12.1)\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting jinja2>=2.10.3 (from distributed==2024.12.1)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting locket>=1.0.0 (from distributed==2024.12.1)\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting msgpack>=1.0.2 (from distributed==2024.12.1)\n",
            "  Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting psutil>=5.8.0 (from distributed==2024.12.1)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting sortedcontainers>=2.0.5 (from distributed==2024.12.1)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tblib>=1.6.0 (from distributed==2024.12.1)\n",
            "  Downloading tblib-3.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting tornado>=6.2.0 (from distributed==2024.12.1)\n",
            "  Downloading tornado-6.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting urllib3>=1.26.5 (from distributed==2024.12.1)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting zict>=3.0.0 (from distributed==2024.12.1)\n",
            "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
            "Collecting zipp>=3.20 (from importlib_metadata>=4.13.0->dask==2024.12.1)\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.10.3->distributed==2024.12.1)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading dask-2024.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distributed-2024.12.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.7/403.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
            "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading tblib-3.1.0-py3-none-any.whl (12 kB)\n",
            "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.5-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.5/442.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: sortedcontainers, zipp, zict, urllib3, tornado, toolz, tblib, pyyaml, psutil, packaging, msgpack, MarkupSafe, locket, fsspec, cloudpickle, click, partd, jinja2, importlib_metadata, dask, distributed\n",
            "  Attempting uninstall: sortedcontainers\n",
            "    Found existing installation: sortedcontainers 2.4.0\n",
            "    Uninstalling sortedcontainers-2.4.0:\n",
            "      Successfully uninstalled sortedcontainers-2.4.0\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.21.0\n",
            "    Uninstalling zipp-3.21.0:\n",
            "      Successfully uninstalled zipp-3.21.0\n",
            "  Attempting uninstall: zict\n",
            "    Found existing installation: zict 3.0.0\n",
            "    Uninstalling zict-3.0.0:\n",
            "      Successfully uninstalled zict-3.0.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 6.4.2\n",
            "    Uninstalling tornado-6.4.2:\n",
            "      Successfully uninstalled tornado-6.4.2\n",
            "  Attempting uninstall: toolz\n",
            "    Found existing installation: toolz 0.12.1\n",
            "    Uninstalling toolz-0.12.1:\n",
            "      Successfully uninstalled toolz-0.12.1\n",
            "  Attempting uninstall: tblib\n",
            "    Found existing installation: tblib 3.1.0\n",
            "    Uninstalling tblib-3.1.0:\n",
            "      Successfully uninstalled tblib-3.1.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: locket\n",
            "    Found existing installation: locket 1.0.0\n",
            "    Uninstalling locket-1.0.0:\n",
            "      Successfully uninstalled locket-1.0.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.1\n",
            "    Uninstalling cloudpickle-3.1.1:\n",
            "      Successfully uninstalled cloudpickle-3.1.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.0\n",
            "    Uninstalling click-8.2.0:\n",
            "      Successfully uninstalled click-8.2.0\n",
            "  Attempting uninstall: partd\n",
            "    Found existing installation: partd 1.4.2\n",
            "    Uninstalling partd-1.4.2:\n",
            "      Successfully uninstalled partd-1.4.2\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2024.12.1\n",
            "    Uninstalling dask-2024.12.1:\n",
            "      Successfully uninstalled dask-2024.12.1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 2024.12.1\n",
            "    Uninstalling distributed-2024.12.1:\n",
            "      Successfully uninstalled distributed-2024.12.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5 which is incompatible.\n",
            "ibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
            "langchain-core 0.3.59 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 click-8.2.0 cloudpickle-3.1.1 dask-2024.12.1 distributed-2024.12.1 fsspec-2025.3.2 importlib_metadata-8.7.0 jinja2-3.1.6 locket-1.0.0 msgpack-1.1.0 packaging-25.0 partd-1.4.2 psutil-7.0.0 pyyaml-6.0.2 sortedcontainers-2.4.0 tblib-3.1.0 toolz-1.0.0 tornado-6.5 urllib3-2.4.0 zict-3.0.0 zipp-3.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata",
                  "psutil",
                  "tornado",
                  "zipp"
                ]
              },
              "id": "69c03c2366aa4cb0b0cb3f99581d2c43"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jzS7V4P_hYrA",
        "outputId": "96e5927b-4b4a-4be7-b555-9c5a38a02b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dask_ml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5484d2529d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_ml'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os, pathlib, PIL, shutil, glob\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.dtypes.api import is_numeric_dtype, is_string_dtype\n",
        "\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark import SparkFiles\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.window as W\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from dask_ml.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from dask.distributed import Client\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler, OneHotEncoder, StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from mlxtend.evaluate import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "KqEjDON6diJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNbyeCbEQT7f"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S35QDVofK5Fp"
      },
      "source": [
        "!kaggle datasets download -d shantanugarg274/sales-dataset\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyHm6PvFLHKG"
      },
      "source": [
        "zip_ref = zipfile.ZipFile('sales-dataset.zip', 'r') #localizar el nombre del archivo .zip y colocarlo\n",
        "zip_ref.extractall('Data') #Extracción de archivos descargados en una carpeta llamada 'files', podria ser cualquier nombre\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"LogReg PySpark\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "fLnNS05LRxsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos = spark.read.csv('/content/Data/Sales Dataset.csv',inferSchema=True, header=True)\n",
        "datos.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "omvOtBXFRfxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos.printSchema()"
      ],
      "metadata": {
        "id": "KawCsFHVaTEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbUJhmVuuHhs"
      },
      "source": [
        "#Informacion Faltante y Columnas innecesarias\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datos_f1 = datos.drop(\"Order ID\")\n",
        "datos_f1.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "P8tWiVhhJj0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faltantes(df):\n",
        "  falta = df.select([F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df.columns])\n",
        "  return falta.show()"
      ],
      "metadata": {
        "id": "-Szo0b8hTw_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faltantes(datos_f1)"
      ],
      "metadata": {
        "id": "IoOxvjQeTy1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis Exploratorio de Datos (EDA)"
      ],
      "metadata": {
        "id": "S2QE1U9GPNhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = [t[0] for t in datos_f1.dtypes if t[1] == 'int' or t[1] == 'double']\n",
        "datos_f1.select(numeric_features).describe().toPandas().transpose()"
      ],
      "metadata": {
        "id": "CxJ450fecC5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_EDA = datos_f1.toPandas()\n",
        "num_list = []\n",
        "cat_list = []\n",
        "\n",
        "fig, axes = plt.subplots(2, len(datos_EDA.columns), figsize=(300, 10))  # Ajusta el tamaño según sea necesario\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Contador para los gráficos\n",
        "graph_count = 0\n",
        "\n",
        "# Iterar sobre las columnas del DataFrame\n",
        "for column in datos_EDA.columns:\n",
        "    if is_numeric_dtype(datos_EDA[column]):\n",
        "        # Histograma en la primera fila\n",
        "        sns.histplot(datos_EDA[column], kde=True, ax=axes[graph_count])\n",
        "        axes[graph_count].set_title(f\"{column}\")\n",
        "\n",
        "        # Boxplot en la segunda fila\n",
        "        sns.boxplot(x=datos_EDA[column], ax=axes[graph_count + len(datos_EDA.columns)])\n",
        "        axes[graph_count + len(datos_EDA.columns)].set_title(f\"{column}\")\n",
        "        num_list.append(column)\n",
        "        graph_count += 1  # Pasar al siguiente gráfico\n",
        "\n",
        "    elif is_string_dtype(datos_EDA[column]):\n",
        "        sns.countplot(data=datos_EDA, x=datos_EDA[column], ax=axes[graph_count])\n",
        "        axes[graph_count].set_title(f\"{column}\")\n",
        "        cat_list.append(column)\n",
        "        axes[graph_count + len(datos_EDA.columns)].axis(\"off\")\n",
        "        graph_count += 1  # Pasar al siguiente gráfico\n",
        "\n",
        "# Mostrar la gráfica\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IzUokGUtdmXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = datos.select(numeric_features).toPandas()\n",
        "sns.pairplot(numeric_data, height=1.4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PPmkGklPgJ3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(numeric_data.corr('spearman'),annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dRUaCQeWgdFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es evidente que no hay variables numéricas altamente correlacionadas. Por lo tanto, las mantendremos todas para el modelo. Sin embargo, las columnas x no son realmente útiles, eliminaremos estas x columnas."
      ],
      "metadata": {
        "id": "iWVujrnRpzwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Modelo 1*\n"
      ],
      "metadata": {
        "id": "RDsYk_lTwlhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import VectorUDT\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VfIEnWeD-SiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datos_f2 = datos_f1.select(\n",
        "    'Order Date', 'City', 'State',\n",
        "    'Category', 'Sub-Category', 'PaymentMode',\n",
        "    'Quantity', 'Amount', 'Profit', 'Year-Month'\n",
        ")\n",
        "\n",
        "cols = datos_f2.columns\n",
        "datos_f2.printSchema()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qn2UewKghECr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Etapa de preprocesamiento\n",
        "\n",
        "salida = 'Profit'\n",
        "caracteristicas = [car for car in cols if car != salida]\n",
        "\n",
        "cat = [c for c, t in datos_f2.dtypes if t == 'string' and c != salida]\n",
        "num = [c for c, t in datos_f2.dtypes if t in ['int', 'double'] and c != salida]\n",
        "\n",
        "print('Cat:', cat, '\\nNum:', num)\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=f\"c_{col}\") for col in cat]\n",
        "encoders = [OneHotEncoder(inputCol=f\"c_{col}\", outputCol=f\"o_{col}\") for col in cat]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=num + [f\"o_{col}\" for col in cat],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "labelIndexer = StringIndexer(inputCol=salida, outputCol='label') if salida in datos_f2.columns else None\n"
      ],
      "metadata": {
        "id": "7Sxk-23dh216"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline de transformación\n",
        "stages = indexers + encoders + [assembler]\n",
        "if labelIndexer:\n",
        "    stages.append(labelIndexer)\n",
        "\n",
        "preprocessor = Pipeline(stages=stages).fit(datos_f2)\n",
        "datos_f3 = preprocessor.transform(datos_f2)\n",
        "\n"
      ],
      "metadata": {
        "id": "z2lbdpC6_O0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar solo las columnas necesarias\n",
        "selectedCols = ['label', 'features']\n",
        "datos_f4 = datos_f3.select(selectedCols)"
      ],
      "metadata": {
        "id": "vNyeJKaU_SGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*División de conjuntos de ajuste (entrenamiento) y prueba*\n"
      ],
      "metadata": {
        "id": "ul86NtHK3UpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en entrenamiento y prueba\n",
        "train, test = datos_f4.randomSplit([0.7, 0.3], seed=2024)\n",
        "print(\"Training Dataset Count:\", train.count())\n",
        "print(\"Test Dataset Count:\", test.count())\n",
        "\n",
        "# -------------------------------\n",
        "# Convertir DenseVector a array\n",
        "# -------------------------------\n",
        "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
        "\n",
        "train = train.withColumn(\"features_array\", vector_to_array_udf(\"features\"))\n",
        "test = test.withColumn(\"features_array\", vector_to_array_udf(\"features\"))\n",
        "\n",
        "# Pasar a Pandas\n",
        "train_pd = train.select(\"features_array\", \"label\").toPandas()\n",
        "test_pd = test.select(\"features_array\", \"label\").toPandas()\n",
        "\n",
        "# Expandir los arrays en columnas individuales\n",
        "X_train = pd.DataFrame(train_pd[\"features_array\"].tolist())\n",
        "X_test = pd.DataFrame(test_pd[\"features_array\"].tolist())\n",
        "y_train = train_pd[\"label\"]\n",
        "y_test = test_pd[\"label\"]\n"
      ],
      "metadata": {
        "id": "1Lts2S8b_c95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVC\n"
      ],
      "metadata": {
        "id": "LbKG23ve_rsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_SVC = SVC()\n",
        "param_grid = {\"C\": [0.01, 0.1, 1.0, 3.0, 10.0],\n",
        "              \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"]\n",
        "              }\n"
      ],
      "metadata": {
        "id": "kvVvBFrS_t9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dask.distributed import Client\n",
        "\n",
        "gs_SVC = RandomizedSearchCV(cls_SVC, param_grid, n_iter=10, scheduler=client, random_state=42).fit(X_train, y_train)\n",
        "print(\"Mejores hiperparámetros:\", gs_SVC.best_params_)"
      ],
      "metadata": {
        "id": "jIlGmCjk_5cS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}